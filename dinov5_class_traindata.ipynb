{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO Self-Supervised Learning Implementation\n",
    "# Using competition pretraining dataset from HuggingFace\n",
    "\n",
    "# Note: Install dependencies with: pip install faiss-cpu torch torchvision datasets\n",
    "\n",
    "# pip install faiss-cpu\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import faiss\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # For plotting loss and k-NN accuracy\n",
    "import copy\n",
    "\n",
    "class DINOTarget:\n",
    "    def __init__(self, dim, momentum=0.9, teacher_temp=0.07, device=\"cuda\"):\n",
    "        # DINO paper uses teacher_temp=0.07 (not 0.04)\n",
    "        self.center = torch.zeros(1, dim, device=device)\n",
    "        self.momentum = momentum\n",
    "        self.teacher_temp = teacher_temp\n",
    "\n",
    "    def __call__(self, teacher_logits):\n",
    "        # center\n",
    "        t = teacher_logits - self.center\n",
    "        # sharpen\n",
    "        t = t / self.teacher_temp\n",
    "        t = F.softmax(t, dim=-1)\n",
    "        # update center\n",
    "        self.center = self.center * self.momentum + (1 - self.momentum) * teacher_logits.mean(dim=0, keepdim=True)\n",
    "        return t.detach()\n",
    "\n",
    "def load_checkpoint(checkpoint_path, student, teacher, student_head, teacher_head, optimizer=None, device=\"cuda\"):\n",
    "    \"\"\"Load a checkpoint and restore model states\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    student.load_state_dict(checkpoint['student_state_dict'])\n",
    "    teacher.load_state_dict(checkpoint['teacher_state_dict'])\n",
    "    student_head.load_state_dict(checkpoint['student_head_state_dict'])\n",
    "    teacher_head.load_state_dict(checkpoint['teacher_head_state_dict'])\n",
    "    if optimizer is not None and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}, k-NN acc: {checkpoint.get('knn_acc', 'N/A')}\")\n",
    "    return checkpoint['epoch'], checkpoint.get('knn_acc', 0.0)\n",
    "\n",
    "def update_teacher(student, teacher, student_head, teacher_head, ema_m):\n",
    "    # Update backbone parameters\n",
    "    for s_param, t_param in zip(student.parameters(), teacher.parameters()):\n",
    "        t_param.data = ema_m * t_param.data + (1 - ema_m) * s_param.data\n",
    "    # Update projection head parameters\n",
    "    for s_param, t_param in zip(student_head.parameters(), teacher_head.parameters()):\n",
    "        t_param.data = ema_m * t_param.data + (1 - ema_m) * s_param.data\n",
    "\n",
    "def dino_loss(student_logits, teacher_probs, student_temp=0.1):\n",
    "    # student_logits: raw output of student projection head (NOT normalized)\n",
    "    # teacher_probs: output of teacher after centering and sharpening\n",
    "    # DINO paper uses student_temp=0.1 (which we have)\n",
    "    # Apply temperature scaling to student logits before log_softmax\n",
    "    student_log_probs = F.log_softmax(student_logits / student_temp, dim=-1)\n",
    "    loss = -(teacher_probs * student_log_probs).sum(dim=-1).mean()\n",
    "    return loss\n",
    "\n",
    "def koleo_loss(embeddings, k=3, eps=1e-8):\n",
    "    \"\"\"\n",
    "    KoLeo (Kozachenko-Leonenko) regularization loss.\n",
    "    Encourages diverse representations by maximizing entropy using k-NN distances.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: Tensor of shape (batch_size, embed_dim) - normalized embeddings\n",
    "        k: Number of nearest neighbors to use\n",
    "        eps: Small epsilon for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "        KoLeo loss (negative entropy, so we minimize it to maximize entropy)\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    # embeddings: (B, D), compute (B, B) distance matrix\n",
    "    dot_product = torch.mm(embeddings, embeddings.t())  # (B, B)\n",
    "    # For normalized vectors, dot product = cosine similarity\n",
    "    # Distance = 1 - cosine_similarity (for normalized vectors)\n",
    "    distances = 1 - dot_product  # (B, B)\n",
    "    \n",
    "    # Set diagonal to large value (self-distance should be ignored)\n",
    "    distances.fill_diagonal_(float('inf'))\n",
    "    \n",
    "    # Find k nearest neighbors for each sample\n",
    "    # Get k smallest distances (excluding self)\n",
    "    knn_distances, _ = torch.topk(distances, k=k, dim=1, largest=False)  # (B, k)\n",
    "    \n",
    "    # KoLeo entropy estimate: -log(knn_distance) averaged\n",
    "    # We want to maximize entropy, so we minimize negative entropy\n",
    "    # Add eps to avoid log(0)\n",
    "    log_distances = torch.log(knn_distances + eps)  # (B, k)\n",
    "    koleo = -log_distances.mean()  # Negative entropy (we minimize this)\n",
    "    \n",
    "    return koleo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dino(train_loader, student, teacher, student_head, teacher_head,\n",
    "               optimizer, device=\"cuda\", num_epochs=50, ema_m=0.996, knn_eval_freq=5,\n",
    "               warmup_epochs=10, save_dir=\"./checkpoints\", save_freq=10, koleo_weight=0.1,\n",
    "               knn_train_loader=None, knn_test_loader=None, resume_from=None):\n",
    "\n",
    "    # Create checkpoint directory\n",
    "    if save_dir:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize or resume from checkpoint\n",
    "    start_epoch = 0\n",
    "    best_knn_acc = 0.0\n",
    "    losses = []\n",
    "    knn_accuracies = []\n",
    "    epochs_evaluated = []\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    # Check for resume checkpoint\n",
    "    resume_checkpoint_path = None\n",
    "    if resume_from:\n",
    "        resume_checkpoint_path = resume_from\n",
    "    elif save_dir:\n",
    "        # Check for latest checkpoint\n",
    "        latest_checkpoint = Path(save_dir) / \"checkpoint_latest.pt\"\n",
    "        if latest_checkpoint.exists():\n",
    "            resume_checkpoint_path = str(latest_checkpoint)\n",
    "    \n",
    "    if resume_checkpoint_path and Path(resume_checkpoint_path).exists():\n",
    "        print(f\"\\nüîÑ Resuming training from {resume_checkpoint_path}...\")\n",
    "        checkpoint = torch.load(resume_checkpoint_path, map_location=device, weights_only=False)\n",
    "        student.load_state_dict(checkpoint['student_state_dict'])\n",
    "        teacher.load_state_dict(checkpoint['teacher_state_dict'])\n",
    "        student_head.load_state_dict(checkpoint['student_head_state_dict'])\n",
    "        teacher_head.load_state_dict(checkpoint['teacher_head_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if 'scaler_state_dict' in checkpoint:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_knn_acc = checkpoint.get('best_knn_acc', 0.0)\n",
    "        losses = checkpoint.get('losses', [])\n",
    "        knn_accuracies = checkpoint.get('knn_accuracies', [])\n",
    "        epochs_evaluated = checkpoint.get('epochs_evaluated', [])\n",
    "        print(f\"‚úì Resumed from epoch {start_epoch}, best k-NN acc: {best_knn_acc:.2f}%\")\n",
    "    else:\n",
    "        # Initialize teacher as a copy of student\n",
    "        teacher.load_state_dict(student.state_dict())\n",
    "        teacher.eval()\n",
    "        # Initialize teacher_head as a copy of student_head\n",
    "        teacher_head.load_state_dict(student_head.state_dict())\n",
    "        print(\"‚úì Starting fresh training\")\n",
    "    \n",
    "    dino_target = DINOTarget(dim=teacher_head.mlp[-1].out_features, device=device)\n",
    "    if resume_checkpoint_path and Path(resume_checkpoint_path).exists():\n",
    "        # Restore DINO target center if available\n",
    "        if 'dino_target_center' in checkpoint:\n",
    "            dino_target.center = checkpoint['dino_target_center'].to(device)\n",
    "    \n",
    "    num_batches = len(train_loader)\n",
    "    # Use constant learning rate of 1e-4 (no scheduler)\n",
    "    constant_lr = 1e-4\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = constant_lr\n",
    "    \n",
    "    # Initialize plot for real-time updates\n",
    "    fig, ax1, ax2 = None, None, None\n",
    "    plot_path = None\n",
    "    if save_dir:\n",
    "        plot_path = f\"{save_dir}/training_curves.png\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        plt.ion()  # Turn on interactive mode\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        start_time = time.time()\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Use constant learning rate (no scheduler)\n",
    "        lr = constant_lr\n",
    "        \n",
    "        # EMA momentum scheduling (DINO paper: constant 0.996 for backbone)\n",
    "        # For stability, keep EMA constant or schedule very slowly\n",
    "        # DINO paper uses constant 0.996, but we can schedule head EMA slightly\n",
    "        # Keep backbone EMA constant at 0.996 to prevent student-teacher divergence\n",
    "        current_ema_m = ema_m  # Constant 0.996 for stability\n",
    "\n",
    "        for batch_idx, (global_crop, local_crops) in enumerate(train_loader):\n",
    "            global_crop = global_crop.to(device)\n",
    "            local_crops = [lc.to(device) for lc in local_crops]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                # Student embeddings (before projection head for KoLeo)\n",
    "                student_global_emb = student(global_crop, return_embedding=True)\n",
    "                student_global = student_head(student_global_emb)\n",
    "                # DO NOT normalize head outputs - they should be raw logits\n",
    "\n",
    "                # Teacher embeddings (no gradient)\n",
    "                with torch.no_grad():\n",
    "                    teacher_global = teacher(global_crop, return_embedding=True)\n",
    "                    teacher_global = teacher_head(teacher_global)\n",
    "                    # DO NOT normalize head outputs - they should be raw logits\n",
    "                    teacher_probs = dino_target(teacher_global)\n",
    "                    \n",
    "                    # Diagnostic: check teacher output variance (should be > 0)\n",
    "                    if batch_idx == 0 and epoch % 20 == 0:\n",
    "                        teacher_var = teacher_global.var(dim=0).mean().item()\n",
    "                        teacher_entropy = -(teacher_probs * torch.log(teacher_probs + 1e-10)).sum(dim=1).mean().item()\n",
    "                        if epoch == 0 or epoch % 50 == 0:\n",
    "                            print(f\"    [Debug] Teacher logits var: {teacher_var:.4f}, Teacher entropy: {teacher_entropy:.4f}\")\n",
    "\n",
    "                # DINO loss for global crop\n",
    "                loss = dino_loss(student_global, teacher_probs)\n",
    "\n",
    "                # Collect embeddings for KoLeo regularization\n",
    "                embeddings_list = [student_global_emb]\n",
    "\n",
    "                # DINO loss for local crops (student only)\n",
    "                for lc in local_crops:\n",
    "                    student_local_emb = student(lc, return_embedding=True)\n",
    "                    student_local = student_head(student_local_emb)\n",
    "                    # DO NOT normalize head outputs - they should be raw logits\n",
    "                    loss += dino_loss(student_local, teacher_probs)\n",
    "                    embeddings_list.append(student_local_emb)\n",
    "                \n",
    "                loss /= (1 + len(local_crops))\n",
    "                \n",
    "                # KoLeo regularization: encourage diverse representations\n",
    "                if koleo_weight > 0:\n",
    "                    # Concatenate all embeddings (global + local crops)\n",
    "                    all_embeddings = torch.cat(embeddings_list, dim=0)  # (B*(1+num_local), embed_dim)\n",
    "                    koleo_reg = koleo_loss(all_embeddings, k=3)\n",
    "                    loss += koleo_weight * koleo_reg\n",
    "\n",
    "            # Backprop\n",
    "            scaler.scale(loss).backward()\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(list(student.parameters()) + list(student_head.parameters()), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # EMA update of teacher (both backbone and head) with scheduled momentum\n",
    "            update_teacher(student, teacher, student_head, teacher_head, current_ema_m)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        losses.append(avg_loss)  # Track loss for plotting\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {lr:.6f}, EMA: {current_ema_m:.4f}, Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Update plot in real-time\n",
    "        if save_dir and len(losses) > 0 and fig is not None:\n",
    "            ax1.clear()\n",
    "            ax1.plot(range(1, len(losses) + 1), losses, 'b-', linewidth=2)\n",
    "            ax1.set_xlabel('Epoch', fontsize=12)\n",
    "            ax1.set_ylabel('Loss', fontsize=12)\n",
    "            ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_xlim([1, max(len(losses), num_epochs)])\n",
    "            \n",
    "            if len(knn_accuracies) > 0:\n",
    "                ax2.clear()\n",
    "                ax2.plot(epochs_evaluated, knn_accuracies, 'r-o', linewidth=2, markersize=6)\n",
    "                ax2.set_xlabel('Epoch', fontsize=12)\n",
    "                ax2.set_ylabel('k-NN Accuracy (%)', fontsize=12)\n",
    "                ax2.set_title('k-NN Accuracy (Teacher)', fontsize=14, fontweight='bold')\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "                ax2.set_xlim([1, num_epochs])\n",
    "                if len(knn_accuracies) > 0:\n",
    "                    ax2.set_ylim([0, max(100, max(knn_accuracies) * 1.1)])\n",
    "            else:\n",
    "                ax2.clear()\n",
    "                ax2.text(0.5, 0.5, 'No k-NN evaluations yet', \n",
    "                        ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "                ax2.set_title('k-NN Accuracy (Teacher)', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "            plt.pause(0.01)  # Small pause to update plot\n",
    "\n",
    "        # k-NN evaluation and checkpoint saving\n",
    "        # DINO paper: evaluate on teacher EMA model only, using [cls] token\n",
    "        knn_acc = None\n",
    "        if (epoch + 1) % knn_eval_freq == 0 or (epoch + 1) == num_epochs:\n",
    "            if knn_train_loader is not None and knn_test_loader is not None:\n",
    "                knn_acc = knn_evaluate(teacher, knn_train_loader, knn_test_loader, k=20, device=device)\n",
    "                knn_accuracies.append(knn_acc)\n",
    "                epochs_evaluated.append(epoch + 1)\n",
    "                print(f\"--- Epoch {epoch+1}, k-NN Accuracy (teacher): {knn_acc:.2f}% ---\")\n",
    "                \n",
    "                # Save best model based on k-NN accuracy\n",
    "                if knn_acc > best_knn_acc:\n",
    "                    best_knn_acc = knn_acc\n",
    "                    if save_dir:\n",
    "                        checkpoint = {\n",
    "                            'epoch': epoch + 1,\n",
    "                            'student_state_dict': student.state_dict(),\n",
    "                            'teacher_state_dict': teacher.state_dict(),\n",
    "                            'student_head_state_dict': student_head.state_dict(),\n",
    "                            'teacher_head_state_dict': teacher_head.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scaler_state_dict': scaler.state_dict(),\n",
    "                            'knn_acc': knn_acc,\n",
    "                            'loss': avg_loss,\n",
    "                            'best_knn_acc': best_knn_acc,\n",
    "                            'dino_target_center': dino_target.center.cpu(),\n",
    "                            'losses': losses,\n",
    "                            'knn_accuracies': knn_accuracies,\n",
    "                            'epochs_evaluated': epochs_evaluated,\n",
    "                        }\n",
    "                        torch.save(checkpoint, f\"{save_dir}/best_model.pt\", _use_new_zipfile_serialization=False)\n",
    "                        print(f\"  ‚Üí Saved best model (k-NN: {knn_acc:.2f}%)\")\n",
    "        \n",
    "        # Save latest checkpoint every epoch (for resuming)\n",
    "        if save_dir:\n",
    "            latest_checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'student_state_dict': student.state_dict(),\n",
    "                'teacher_state_dict': teacher.state_dict(),\n",
    "                'student_head_state_dict': student_head.state_dict(),\n",
    "                'teacher_head_state_dict': teacher_head.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'knn_acc': knn_acc,\n",
    "                'loss': avg_loss,\n",
    "                'best_knn_acc': best_knn_acc,\n",
    "                'dino_target_center': dino_target.center.cpu(),\n",
    "                'losses': losses,\n",
    "                'knn_accuracies': knn_accuracies,\n",
    "                'epochs_evaluated': epochs_evaluated,\n",
    "            }\n",
    "            torch.save(latest_checkpoint, f\"{save_dir}/checkpoint_latest.pt\", _use_new_zipfile_serialization=False)\n",
    "        \n",
    "        # Periodic checkpoint saving (numbered checkpoints)\n",
    "        if save_dir and (epoch + 1) % save_freq == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'student_state_dict': student.state_dict(),\n",
    "                'teacher_state_dict': teacher.state_dict(),\n",
    "                'student_head_state_dict': student_head.state_dict(),\n",
    "                'teacher_head_state_dict': teacher_head.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict(),\n",
    "                'knn_acc': knn_acc,\n",
    "                'loss': avg_loss,\n",
    "                'best_knn_acc': best_knn_acc,\n",
    "                'dino_target_center': dino_target.center.cpu(),\n",
    "                'losses': losses,\n",
    "                'knn_accuracies': knn_accuracies,\n",
    "                'epochs_evaluated': epochs_evaluated,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{save_dir}/checkpoint_epoch_{epoch+1}.pt\", _use_new_zipfile_serialization=False)\n",
    "            print(f\"  ‚Üí Saved checkpoint: checkpoint_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    # Final plot update\n",
    "    if save_dir and len(losses) > 0 and fig is not None:\n",
    "        plt.ioff()  # Turn off interactive mode\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\n‚úì Final training curves saved to {plot_path}\")\n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Model Architecture\n",
    "# ======================================================================\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.mha(x, x, x)\n",
    "        return out\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.w_out = nn.Linear(hidden_dim, input_dim)\n",
    "    def forward(self, x):\n",
    "        return self.w_out(self.w1(x) * F.silu(self.w2(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(SwiGLU(embed_dim, mlp_dim), nn.Dropout(dropout))\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x)))\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, out_dim)\n",
    "        )\n",
    "        # Remove LayerNorm - we want raw logits, not normalized outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.patch_embedding = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
    "    num_patches = (image_size // patch_size) ** 2\n",
    "    self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.transformer_blocks = nn.ModuleList([\n",
    "      TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)\n",
    "    ])\n",
    "    self.norm = nn.LayerNorm(embed_dim)\n",
    "    self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "\n",
    "  def forward(self, x, return_embedding: bool = False):\n",
    "    batch_size = x.shape[0]\n",
    "    x = self.patch_embedding(x)\n",
    "    cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "    x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "    # interpolate pos embedding if sizes don't match\n",
    "    if x.size(1) != self.pos_embedding.size(1):\n",
    "        pos_embed = self.pos_embedding[:, 1:, :].transpose(1,2)  # (1, embed_dim, num_patches)\n",
    "        H = W = int((x.size(1)-1) ** 0.5)\n",
    "        pos_embed = pos_embed.reshape(1, x.size(2), int(pos_embed.size(2) ** 0.5), int(pos_embed.size(2) ** 0.5))\n",
    "        pos_embed = F.interpolate(pos_embed, size=(H,W), mode='bicubic', align_corners=False)\n",
    "        pos_embed = pos_embed.flatten(2).transpose(1,2)\n",
    "        pos_embed = torch.cat([self.pos_embedding[:, :1, :], pos_embed], dim=1)  # prepend cls token\n",
    "    else:\n",
    "        pos_embed = self.pos_embedding\n",
    "\n",
    "    x = x + pos_embed\n",
    "    x = self.dropout(x)\n",
    "    for block in self.transformer_blocks:\n",
    "        x = block(x)\n",
    "    x = self.norm(x)\n",
    "    cls_token_output = x[:, 0]  # Extract CLS token (first token) - used for k-NN evaluation\n",
    "    if return_embedding:\n",
    "        return cls_token_output  # Return CLS token embedding only\n",
    "    logits = self.head(cls_token_output)\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# k-NN Evaluation Function (optional - only used if eval dataset provided)\n",
    "# ======================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def knn_evaluate(model, train_loader, test_loader, k, device):\n",
    "    model.eval()\n",
    "    # 1. Build feature bank using CLS token embeddings\n",
    "    # model(images, return_embedding=True) returns the CLS token (first token) from VisionTransformer\n",
    "    features_list, labels_list = [], []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        feats = model(images, return_embedding=True)  # Returns CLS token embedding\n",
    "        feats = F.normalize(feats, dim=1)\n",
    "        features_list.append(feats.cpu())\n",
    "        labels_list.append(labels)\n",
    "    train_features = torch.cat(features_list, dim=0).numpy().astype('float32')\n",
    "    train_labels = torch.cat(labels_list, dim=0).numpy().astype('int64')\n",
    "\n",
    "    # 2. Build FAISS index\n",
    "    d = train_features.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(train_features)\n",
    "\n",
    "    total_correct, total_samples = 0, 0\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.numpy()\n",
    "        feats = model(images, return_embedding=True)  # Returns CLS token embedding\n",
    "        feats = F.normalize(feats, dim=1).cpu().numpy().astype('float32')\n",
    "        D, I = index.search(feats, k)\n",
    "        neighbor_labels = train_labels[I]\n",
    "        preds = []\n",
    "        for nb in neighbor_labels:\n",
    "            vals, counts = np.unique(nb, return_counts=True)\n",
    "            preds.append(vals[np.argmax(counts)])\n",
    "        preds = np.array(preds)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.shape[0]\n",
    "\n",
    "    return 100 * total_correct / total_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ################################################################################\n",
    "    ############################# DATASET SETUP ####################################\n",
    "    ################################################################################\n",
    "\n",
    "    # Load competition pretraining dataset from HuggingFace\n",
    "    image_size = 96  # Competition requirement: 96x96 images\n",
    "    \n",
    "    # OPTIMIZATION: Use subset for faster iteration during development\n",
    "    # Set to None to use full dataset, or specify number (e.g., 50000 for 50k images)\n",
    "    # Full dataset: ~500k images = 2-5 hours/epoch with batch_size=32\n",
    "    # 50k subset: ~20-30 min/epoch (good for testing hyperparameters)\n",
    "    DATASET_SUBSET_SIZE = 50000  # None = full dataset, or set to e.g., 50000, 100000\n",
    "    \n",
    "    print(\"Loading competition pretraining dataset from HuggingFace...\")\n",
    "    print(\"Dataset: tsbpp/fall2025_deeplearning (pretrain split)\")\n",
    "    \n",
    "    # Load the pretraining dataset (unlabeled)\n",
    "    pretrain_dataset = load_dataset(\"tsbpp/fall2025_deeplearning\", split=\"train\")\n",
    "    \n",
    "    # Optionally use a subset for faster iteration\n",
    "    if DATASET_SUBSET_SIZE is not None:\n",
    "        print(f\"‚ö†Ô∏è  Using SUBSET of {DATASET_SUBSET_SIZE:,} images for faster iteration\")\n",
    "        pretrain_dataset = pretrain_dataset.select(range(min(DATASET_SUBSET_SIZE, len(pretrain_dataset))))\n",
    "    \n",
    "    print(f\"Loaded {len(pretrain_dataset):,} unlabeled images for pretraining\")\n",
    "    print(f\"Dataset features: {pretrain_dataset.features}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # DINO-style SSL dataset\n",
    "    # ----------------------------\n",
    "    # OPTIMIZATION: Reduce num_local_crops to speed up training\n",
    "    # - 4 local crops: Standard DINO (slower, better quality)\n",
    "    # - 2 local crops: ~40% faster, slightly less effective\n",
    "    # - 1 local crop: ~60% faster, less effective\n",
    "    num_local_crops = 4  # Standard DINO (set to 2 for faster training)\n",
    "    \n",
    "    class DINODataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, hf_dataset, image_size=96, num_local_crops=4):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                hf_dataset: HuggingFace dataset (returns dict with 'image' key)\n",
    "                image_size: Target image size\n",
    "                num_local_crops: Number of local crops per image\n",
    "            \"\"\"\n",
    "            self.dataset = hf_dataset\n",
    "            self.num_local_crops = num_local_crops\n",
    "            \n",
    "            # Global crop transforms (DINO-style augmentations)\n",
    "            global_transforms = [\n",
    "                transforms.RandomResizedCrop(image_size, scale=(0.5, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.5),\n",
    "                transforms.RandomApply([transforms.RandomSolarize(threshold=128, p=1.0)], p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "            self.global_transform = transforms.Compose(global_transforms)\n",
    "            \n",
    "            # Local crop transforms\n",
    "            local_transforms = [\n",
    "                transforms.RandomResizedCrop(image_size, scale=(0.14, 0.5)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.RandomApply([transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.5),\n",
    "                transforms.RandomApply([transforms.RandomSolarize(threshold=128, p=1.0)], p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "            self.local_transform = transforms.Compose(local_transforms)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # HuggingFace dataset returns dict with 'image' key (PIL Image)\n",
    "            item = self.dataset[idx]\n",
    "            img = item['image']\n",
    "            \n",
    "            # Convert to RGB if needed\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Resize to ensure minimum size for cropping\n",
    "            if min(img.size) < image_size:\n",
    "                img = transforms.Resize(image_size, interpolation=InterpolationMode.BICUBIC)(img)\n",
    "            \n",
    "            # Apply transforms\n",
    "            global_crop = self.global_transform(img)\n",
    "            local_crops = [self.local_transform(img) for _ in range(self.num_local_crops)]\n",
    "            \n",
    "            return global_crop, local_crops\n",
    "\n",
    "    # Create SSL dataset from HuggingFace pretraining data\n",
    "    ssl_ds_train = DINODataset(pretrain_dataset, image_size=image_size, num_local_crops=num_local_crops)\n",
    "    print(f\"Using {num_local_crops} local crops per image (1 global + {num_local_crops} local = {num_local_crops+1} total crops)\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # DataLoaders\n",
    "    # ----------------------------\n",
    "    num_cores = os.cpu_count() or 2\n",
    "    print(f\"Using {num_cores} workers for DataLoaders.\")\n",
    "\n",
    "    # SMALL GPU VERSION: Reduced batch size for limited GPU memory\n",
    "    # DINO authors: LR is most sensitive hyperparameter, scale with batch size\n",
    "    # Smaller batch sizes need proportionally smaller learning rates\n",
    "    batch_size = 32  # SMALL GPU VERSION: Reduced for limited GPU memory\n",
    "    # Try increasing to 64 or 128 if you have more GPU memory (faster training)\n",
    "    \n",
    "    # Calculate estimated epoch time\n",
    "    iterations_per_epoch = len(ssl_ds_train) / batch_size\n",
    "    estimated_hours = iterations_per_epoch * 0.8 / 3600  # Rough estimate: 0.8s per iteration\n",
    "    print(f\"\\nüìä Training info:\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Iterations per epoch: ~{iterations_per_epoch:,.0f}\")\n",
    "    print(f\"   Estimated time per epoch: ~{estimated_hours:.1f} hours (varies by GPU)\")\n",
    "    print(f\"   Tip: Reduce DATASET_SUBSET_SIZE or num_local_crops to speed up\")\n",
    "    \n",
    "    train_loader = DataLoader(ssl_ds_train, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_cores, pin_memory=True)\n",
    "\n",
    "    dataset_name = \"Competition Pretraining Dataset (HuggingFace)\"\n",
    "    print(f\"{dataset_name} loaded. SSL Train: {len(ssl_ds_train)} images\")\n",
    "\n",
    "    ################################################################################\n",
    "    ############################# MODEL SETUP ######################################\n",
    "    ################################################################################\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # --- Vision Transformer student + teacher ---\n",
    "    # SMALL GPU VERSION: Reduced parameters for limited GPU memory\n",
    "    # With 96x96 images: patch_size=8 gives 144 patches (vs 576 with patch_size=4)\n",
    "    embed_dim = 256  # SMALL GPU VERSION: Reduced from 512\n",
    "    model = VisionTransformer(\n",
    "        image_size=image_size,  # 96x96 for competition\n",
    "        patch_size=8,  # SMALL GPU VERSION: Larger patches = fewer: (96/8)^2 = 144 patches\n",
    "        in_channels=3,\n",
    "        embed_dim=embed_dim,\n",
    "        num_heads=8,    # embed_dim must be divisible by num_heads (256/8 = 32)\n",
    "        mlp_dim=1024,   # SMALL GPU VERSION: Reduced (4x embed_dim)\n",
    "        num_layers=6,  # SMALL GPU VERSION: Reduced from 12\n",
    "        num_classes=100,  # not used for SSL\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    student = model\n",
    "    teacher = copy.deepcopy(student)\n",
    "    teacher.eval()\n",
    "\n",
    "    # --- Projection heads ---\n",
    "    # DINO paper: use >=65k prototypes for best results\n",
    "    # Using 65536 (2^16) for optimal performance - this is critical for stability\n",
    "    projection_dim = 65536\n",
    "    student_head = DINOHead(in_dim=embed_dim, out_dim=projection_dim).to(device)\n",
    "    teacher_head = DINOHead(in_dim=embed_dim, out_dim=projection_dim).to(device)\n",
    "\n",
    "    # --- Optimizer ---\n",
    "    # Use constant learning rate of 1e-4 (no scheduler, no batch size scaling)\n",
    "    learning_rate = 1e-4\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(student.parameters()) + list(student_head.parameters()),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.04,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    print(f\"Using constant learning rate: {learning_rate:.6f}\")\n",
    "\n",
    "    total_params = sum(p.numel() for p in student.parameters())\n",
    "    trainable_params = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params}, Trainable parameters: {trainable_params}\")\n",
    "\n",
    "    ################################################################################\n",
    "    ############################# TRAINING ########################################\n",
    "    ################################################################################\n",
    "\n",
    "    print(\"Start Time:\" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "    print(\"Starting DINO training...\")\n",
    "\n",
    "    # DINO paper trains for 300 epochs on ImageNet\n",
    "    # For competition dataset, we'll use 200 epochs\n",
    "    # Optional: Resume from a specific checkpoint\n",
    "    # Set resume_from=None to start fresh, or provide path like \"./checkpoints/checkpoint_latest.pt\"\n",
    "    resume_from = None  # Change to checkpoint path if you want to resume from a specific checkpoint\n",
    "    \n",
    "    train_dino(\n",
    "        train_loader,\n",
    "        student,\n",
    "        teacher,\n",
    "        student_head,\n",
    "        teacher_head,\n",
    "        optimizer,\n",
    "        device=device,\n",
    "        num_epochs=200,  # DINO paper uses 300, but smaller datasets need fewer epochs\n",
    "        ema_m=0.996,\n",
    "        knn_eval_freq=20,  # k-NN evaluation disabled (no eval dataset)\n",
    "        warmup_epochs=10,  # Not used anymore (constant LR), but kept for compatibility\n",
    "        save_dir=\"./checkpoints\",  # Save checkpoints here\n",
    "        save_freq=10,  # Save checkpoint every 10 epochs\n",
    "        koleo_weight=0.1,  # DINOv2: KoLeo regularization weight (0.1 is a good default)\n",
    "        knn_train_loader=None,  # No k-NN evaluation dataset\n",
    "        knn_test_loader=None,  # No k-NN evaluation dataset\n",
    "        resume_from=resume_from  # Resume from checkpoint if interrupted\n",
    "    )\n",
    "\n",
    "    print(\"End Time:\" + time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
